{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning Architectures for Image Classification <br/>and Object Detection\n",
    "\n",
    "**Nelson Correa, *Ph.D.*** <br/>\n",
    "*Data Science, Machine Learning and Natural Language Processing* <br/>\n",
    "[https://linkedin.com/in/ncorrea](https://linkedin.com/in/ncorrea) \n",
    "\n",
    "**[Palm Beach Data Science Meetup](https://www.meetup.com/Palm-Beach-Data-Meetup/events/262988444/)** \n",
    "West Palm Beach, FL\n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Introduction: Computer vision and deep neural networks\n",
    "2. Computer vision tasks and datasets\n",
    "3. Deep neural network advances since 2011\n",
    "4. MLP and CNN: Deep neural networks (MNIST)\n",
    "5. Image classification with VGG16 (ImageNet classifier)\n",
    "6. Object detection with YOLOv3 (MS-COCO object detector)\n",
    "7. New developments and conclusion\n",
    "\n",
    "Aiming for 45 minutes of presentation and 10 - 15 minutes of Q&A.\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Deep Learning Architectures for Image Classification and Object Detection\n",
    "\n",
    "**Nelson Correa, *Ph.D.*** <br/>\n",
    "*Data Science, Machine Learning and Natural Language Processing* <br/>\n",
    "[https://linkedin.com/in/ncorrea](https://linkedin.com/in/ncorrea)\n",
    "[@nelscorrea](https://twitter.com/nelscorrea)\n",
    "<br/>\n",
    "\n",
    "### Abstract\n",
    "\n",
    "Object detection is a task in computer vision with many practical applications that can now be achieved with super-human levels of performance on selected benchmarks using deep neural networks.\n",
    "In this talk we define the *object detection* task and present J. Redmon's YOLO (You Only Look Once) V3 deep neural network architecture.\n",
    "As preliminaries to object detection and YOLOv3, we first describe image classification on the Pascal VOC and ImageNet benchmark datasets, and introduce a series of deep learning neural network architectures that include the multilayer perceptron (MLP), convolutional neural networks (CNNs), and other networks with dystopian names such as AlexNet, GoogLeNet/Inception, VGG16, ResNet, and Region-CNN (R-CNN). \n",
    "We conclude with note of recent developments, including *capsule networks* (CapNets) by G. Hinton and deep networks with visual feedback.\n",
    "Slides and notebooks with code will be available after the talk.\n",
    "\n",
    "### Speaker Bio\n",
    "\n",
    "Nelson Correa is a data scientist and machine learning consultant based in West Palm Beach. \n",
    "He has a Ph.D. in Electrical Engineering and over 25 years of experience in natural language processing at three startup companies and IBM Research. \n",
    "He has over 30 technical publications and three U.S. patents. \n",
    "His current interest is in developing connections between deep learning and symbolic models for natural language processing and perception.\n",
    "\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "### Summary\n",
    "\n",
    "* *Object detection* and J. Redmon's YOLO-V3 (You Only Look Once) <br/> deep neural network.\n",
    "* Image classification on the Pascal VOC and ImageNet datasets\n",
    "* Series of deep learning neural network architectures that include the multilayer perceptron (MLP), convolutional neural networks (CNNs), and other networks such as AlexNet, GoogLeNet/Inception, VGG16, ResNet, and Region-CNN. \n",
    "* Note of recent developments, including *capsule networks* (CapNets) and deep networks with visual feedback.\n",
    "\n",
    "Slides and notebooks with code available after the talk.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### Tools:\n",
    "\n",
    "* TensorFlow, Keras, Scikit-Learn (PyTorch to be done)\n",
    "* Deep learning networks: MLP, CNN, VGG, YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Introduction: A brief chronology\n",
    "\n",
    "### Computer vision and deep neural networks (1950 - 2000)\n",
    "\n",
    "1950s - R. Rosenblatt, W. McCoullogh, W. Pitts (perceptron); Hubel, Weisel (cats)\n",
    "\n",
    "1960s - MIT AI: Minsky, Waltz, Winston, Marr; AI, Blocks World, Robotics\n",
    "\n",
    "1970s - MIT AI & computer vision; E. Harth, Alopex; G. Hinton, connectionist networks\n",
    "\n",
    "1980s - Symbolic AI & computer vision; Connectionism\n",
    "* Rumelhart *et al.*, Parallel Distributed Processing (PDP)\n",
    "* G. Hinton & others, MLP, Backpropagation\n",
    "* Y. LeCun & others, Convolutional neural networks (CNN)\n",
    "\n",
    "1990s - Pattern recognition (PAMI), machine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computer vision and deep neural networks (2000 - present)\n",
    "\n",
    "2000s - Deep learning, OpenCV\n",
    "* Bengio; neural language models\n",
    "* Hinton, LeCun, Bengio & others; training deep networks\n",
    "* G. Bradsky, OpenCV\n",
    "* Stanford, Toronto, MILO, Oxford; Datasets and benchmarks (Pascal VOC, CIFAR)\n",
    "\n",
    "2010-2019 - Rapid progress in deep learning\n",
    "* Google, Microsoft, Facebook; Benchmarks, datasets (ILSVCR, MS-COCO)\n",
    "* AlexNet on ImageNet/ILSVRC, 2012; Bengio, ReLU\n",
    "* Deep network modules, architectures, and toolkits\n",
    "* Hinton, LeCun, Bengio (ACM Turing Award, 2019)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Computer vision tasks and datasets\n",
    "\n",
    "*\"The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. \"* <br/>\n",
    "(https://en.wikipedia.org/wiki/Computer_vision#Recognition)\n",
    "\n",
    "* The terminology varies and includes the terms **recognition**, **classification**, **identification** and **detection** when used to describe computer vision tasks.\n",
    "* A finite set of **labels** or classes (e.g., cat, dog, person, cup, lawn) is assumed to be associated with images, objects and other features of images.\n",
    "* The denotations of the *labels* may be exclusive (disjoint), or they may overlap and define a hierarchy (e.g., of concepts).\n",
    "\n",
    "\n",
    "### NOTES: \n",
    "* Instead of finite, symbolic *labels*, we may define image or class ***attributes*** (in a discrete or continuous space).\n",
    "* The *classical problem* is assumed to be a **supervised** learning problem - correct label or labels for an image given *by example* (an *<image, labels>* training set).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 Image Classification (logistic regression)\n",
    "\n",
    "* Given an **image** and a set of **labels** or classes, determine what *labels* should be assigned to the image.\n",
    "* The problem may be **multi-class** (multinomial classification; more than two classes)\n",
    "* The problem may be **multi-label** (each image may assigned multiple labels)\n",
    "\n",
    "#### Pascal VOC (20 classes) - MS COCO (80 classes):\n",
    "* Person: person\n",
    "* Animal: bird, cat, cow, dog, horse, sheep\n",
    "* Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\n",
    "* Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n",
    "\n",
    "<img src=\"./images/ms_coco_vs_voc_labels.png\" alt=\"MS COCO Labels\" width=\"750\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Object Detection (logistic+linear regression)\n",
    "\n",
    "* Given an **image** and a set of **object labels**, \n",
    "  * (i) determine what **labels** should be assigned to the image (objects in image)\n",
    "  * (ii) indicate the **bounding box** (location) of each object in the image\n",
    "  * (iii) provide a confidence score (**objectness**) for each label selected\n",
    "* *Bounding box* represented by four coordinates *(x, y, width, height)*, relative to image.\n",
    "\n",
    "#### Object Bounding Boxes\n",
    "\n",
    "<img src=\"./images/PBDS_Meetup_crop.jpg\" alt=\"MS COCO Labels\" width=\"500\"/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**NOTE**: Other *CV* and *multi-modal CV* tasks (captioning, Visual Question Answering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 2.3 Other computer vision tasks\n",
    "\n",
    "* Character recognition\n",
    "* Object localization\n",
    "* Object segmentation\n",
    "* Object/Person identification\n",
    "* Object/Person verification\n",
    "\n",
    "\n",
    "## 2.4 Multi-modal CV tasks (vision and language)\n",
    "\n",
    "* Image captioning, description\n",
    "* Visual/Image Question Answering (VQA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5 Computer vision datasets\n",
    "\n",
    "### Datasets\n",
    "* **MNIST** (60,000 28x28x1 handwritten digits 0-9; AT&T Bell Labs; 1989-1998)\n",
    "* **Pascal VOC** (12,500 images; 20 classes; U. Oxford; 2005-2012)\n",
    "* **CIFAR** (60,000 32x32x3 images; 10 or 100 classes; U. Toronto; 2009)\n",
    "* **MS-COCO** (200,000 images; 80 classes; Microsoft; 2014-2019)\n",
    "* **ImageNet** (14M images; 21,000 classes; Google/Stanford; 2010-2019)\n",
    "\n",
    "... other\n",
    "\n",
    "### Image types\n",
    "* Iconic objects and scenes\n",
    "* Non-iconics object and scenes (common objects in context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/imagenet_logo_bk.png\" alt=\"imagenet_logo\" width=\"250\" align=\"left\"/>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\"*ImageNet is **an image database organized according to the WordNet hierarchy** (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.*\"\n",
    "* ImageNet (ILSVRC): http://image-net.org\n",
    "* WordNet: http://wordnet.princeton.edu/\n",
    "* *ImageNet Large Scale Visual Recognition Challenge*, Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei, International Journal of Computer Vision, 2015.\n",
    "https://arxiv.org/abs/1409.0575\n",
    "\n",
    "### ImageNet Summary and Statistics (updated on April 30, 2010)\n",
    "\n",
    "* Total number of non-empty synsets: 21,841\n",
    "* Total number of images: 14,197,122\n",
    "* Number of images with bounding box annotations: 1,034,908\n",
    "* Number of synsets with SIFT features: 1000\n",
    "* Number of images with SIFT features: 1.2 million\n",
    "\n",
    "Source: http://image-net.org/about-stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Deep neural network advances since 2011\n",
    "\n",
    "\n",
    "## 3.1 Deep neural networks pre-2011\n",
    "* Perceptron (trainable; direct solutions or gradient descent). \n",
    "Too simple a learning function (cannot learn XOR; Minsky and Papert, 1969)\n",
    "\n",
    "* Multi-Layer Perceptron (MLP) and deep networks (backpropagation ~ chain rule)\n",
    "  * ... but deep MLP not practically trainable (vanishing gradients, complexity)\n",
    "  * simple architecture: not hierarchical; activation function; feed-forward\n",
    "    * no feedback; no \"attention\" mechanisms\n",
    "\n",
    "* Convolutional neural networks (CNNs) with a few layers (LeCun, 1989)\n",
    "  * CNNs provide *hierarchy*, an advance over fully-connected layers (MLP)\n",
    "    * spatial and visual feature/concept organization\n",
    "  * But deep CNNs were not yet practical (i.e., trainable) in 1989\n",
    "\n",
    "* Parallel Distributed Processing\n",
    "  * CV as combined symbolic AI and pattern recognition\n",
    "  * IEEE PAMI, OpenCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Deep neural network advances since 2011\n",
    "\n",
    "* Improved deep network training (2011 ILSVRC visual recognition challenge)\n",
    "  * More effective neuron activation functions \n",
    "    * (from *sigmoid* to *tanh, reLU, leaky reLU*)\n",
    "  * New training methods (SGD, batch/mini-batch SGD; scaling; optimizers)\n",
    "  * Regularization (network weight rescaling; dropout; data augmentation)\n",
    "  * Reusable transfer learning\n",
    "\n",
    "* Improved deep network models and modules (2011 - present)\n",
    "  * Various new neural network modules (Inception, Xception, Capsules)\n",
    "  * Residual neural networks (ResNet)\n",
    "  * Deep, large network architectures for computer vision (LeNet, GoogLeNet/Inception, VGG, R-CNN, YOLO)\n",
    "  * Attention, feedback (Feedback-CNN) and capsules (CapsNet)\n",
    "\n",
    "* Large image datasets, CV tasks\n",
    "  * Image tasks and datasets: CIFAR, Pascal VOC, MS COCO, ImageNet\n",
    "  * Transfer learning on deep networks trained on the large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. MLP and CNN: Deep neural networks (MNIST)\n",
    "\n",
    "**MNIST dataset** \n",
    "* *The MNIST database of handwritten digits*, Yann LeCun, Corinna Cortes, and Christopher JC Burges, 1998\n",
    "* [LeCun site](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "\n",
    "**MNIST MLP**\n",
    "* Input (28, 28) - Dense-256-relu - Dense-10-softmax\n",
    "* Input (28, 28) - Dense-128-relu - Dense-128-relu - Dense-10-softmax\n",
    "\n",
    "\n",
    "**MNIST CNN**\n",
    "* **Feature extractor**: Input (28, 28), Conv2D-32, Conv2D-64-relu, Conv2D-64-relu\n",
    "  * **Output classifier**: Flatten, Dense-128-relu, Dense-10-softmax\n",
    "\n",
    "\n",
    "See notebook: [MNIST_Digit_Classification.ipynb](./MNIST_Digit_Classification.ipynb) - \n",
    "[html](./MNIST_Digit_Classification.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4.1 MNIST State-of-the-Art (SOTA)\n",
    "\n",
    "### MNIST Test accuracy stands at 99.83% (in 2019)\n",
    "\n",
    "#### Record (03/2019)\n",
    "* Zhao et al., 2019, report absolute MNIST error rate reduction from previous best 0.21% (99.79% accuracy) to 0.17% (99.83% accuracy).\n",
    "* *Capsule Networks with Max-Min Normalization*, \n",
    "Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P. Unnikrishnan, 2019, \n",
    "https://arxiv.org/abs/1903.09662.\n",
    "\n",
    "#### Others (2017 -  2018)\n",
    "* *Regularization of neural networks using dropconnect*, Li Wan, Matthew D Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus, ICML 2013. <br/>\n",
    "99.79% accuracy (0.39% error rate; 0.21% with ensembling)\n",
    "\n",
    "* *Dynamic Routing Between Capsules*,\n",
    "Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, 2017 \n",
    "(https://arxiv.org/abs/1710.09829)\n",
    "99.75% accuracy (0.25% error rate)\n",
    "\n",
    "* Simple 3-layer CNN above (total params, 130,890) <br/>\n",
    "without any training regularization: `acc: 99.15%`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. VGG16 Image classification (ImageNet)\n",
    "\n",
    "\n",
    "<img src=\"./images/vgg16_architecture_01.png\" alt=\"VGG16 architecture\" width=\"700\">\n",
    "<img src=\"./images/vgg16_architecture_02.png\" alt=\"VGG16 architecture\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.1 Image classification with VGG16\n",
    "\n",
    "\n",
    "See notebook: [VGG16_Image_Classification.ipynb](./VGG16_Image_Classification.ipynb) - \n",
    "[html](./VGG16_Image_Classification.html)\n",
    "\n",
    "\n",
    "### Image classification examples and issues with VGG16 in Keras\n",
    "\n",
    "Image classification task\n",
    "* Multiclass; Multilabel\n",
    "\n",
    "VGG16 image classification architecture\n",
    "* Deep CNN with output classifier\n",
    "* 16 trainable layers (13 convolutional; 3 FC classifier); 23 total layers\n",
    "\n",
    "Pretrained and custom VGG16 image classifiers\n",
    "* Pretrained VGG16: 1000 ImageNet classes\n",
    "* custom trained output classifier: Dogs vs. cats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.2 VGG16 Example - African elephant\n",
    "\n",
    "### African elephant (Flickr)\n",
    "\n",
    "<img src=\"./images/african_elephant.jpg\" alt=\"African elephant\" width=\"600\">\n",
    "\n",
    "#### Top-5 Predictions\n",
    "    n02504458 - African_elephant 69.72%\n",
    "    n01871265 - tusker       19.20%\n",
    "    n02504013 - Indian_elephant 6.60%\n",
    "    n02410509 - bison        3.56%\n",
    "    n02437312 - Arabian_camel 0.43%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 5.2 VGG16 Example - African elephant\n",
    "\n",
    "\n",
    "### Dog, Bike and Truck (J. Redmon)\n",
    "\n",
    "<img src=\"./images/dog.jpg\" alt=\"Dog, Bike and Truck\" width=\"300\">\n",
    "\n",
    "#### Top-5 Predictions\n",
    "    n02110063 - malamute     32.37%\n",
    "    n02110185 - Siberian_husky 21.75%\n",
    "    n02109961 - Eskimo_dog   15.27%\n",
    "    n03218198 - dogsled      5.32%\n",
    "    n02106166 - Border_collie 4.22%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.2 VGG16 Example - People\n",
    "\n",
    "### Young man and woman (MS-COCO)\n",
    "\n",
    "<img src=\"./images/man_woman.jpg\" alt=\"Young man and woman\" width=\"300\">\n",
    "\n",
    "#### Top-5 Predictions\n",
    "    n10148035 - groom        39.19%\n",
    "    n02883205 - bow_tie      27.01%\n",
    "    n04350905 - suit         11.94%\n",
    "    n03450230 - gown         8.47%\n",
    "    n03770439 - miniskirt    2.00%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.3 Other image classification architectures\n",
    "\n",
    "\n",
    "## AlexNet (Krishevzky *et al.*, 2012, ILSVRC)\n",
    "\n",
    "AlexNet_architecture_01.png\n",
    "<img src=\"./images/AlexNet_architecture_01.png\" alt=\"AlexNet architecture\" width=\"800\">\n",
    "\n",
    "Input\n",
    "Conv2D\n",
    "Max-Pooling\n",
    "Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.4 Other image classification architectures\n",
    "\n",
    "## Inception V3 (2015)\n",
    "\n",
    "<img src=\"./images/inceptionv3_architecture_01.png\" alt=\"InceptionV3 architecture\" width=\"800\">\n",
    "\n",
    "* https://cloud.google.com/tpu/docs/inception-v3-advanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5.5 Deep learning architecture improvements\n",
    "\n",
    "\n",
    "## InceptionV1 (2014), ResNet, ResNeXt (2016)\n",
    "\n",
    "\n",
    "<img src=\"./images/InceptionV1_module.png\" alt=\"InceptionV1 module\" width=\"350\" align=\"left\">\n",
    "<img src=\"./images/ResNet_ResNeXt_blocks.png\" alt=\"ResNet architecture\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Object Detection\n",
    "\n",
    "An extension of *image classification*.\n",
    "\n",
    "\n",
    "## Image classification\n",
    "\n",
    "**input image** -> **class probabilities** (multinomial logistic regression)\n",
    "\n",
    "\n",
    "## Object localization and detection\n",
    "\n",
    "(single object): **input image** ->\n",
    "* **class probabilities** (multinomial logistic regression), \n",
    "* **bounding box** (x, y, width, height)\n",
    "\n",
    "(multiple objects at different scales): **input image** -> plural image grid cells\n",
    "* **class scores/probabilities** (multinomial logistic regression),\n",
    "* **objecness score** for cell,\n",
    "* **plural bounding boxes** (x, y, width, height)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.1 YOLOv1 Object detection\n",
    "\n",
    "* YOLOv1 (Pascal VOC, 20 classes)\n",
    "* YOLOv3 (MS-COCO, 80 classes) - Total yolov3.layers: 252 (Convolutional: 106)\n",
    "\n",
    "<img src=\"./images/YOLO_Object_detection_01.png\" alt=\"YOLOv1 Object detection\" width=\"750\">\n",
    "\n",
    "<br/>\n",
    "\n",
    "See notebook: [YOLOv3_Object_Detection.ipynb](./YOLOv3_Object_Detection.ipynb) - \n",
    "[html](./YOLOv3_Object_Detection.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6.2 YOLOv3 Object detection (on MS-COCO)\n",
    "\n",
    "\n",
    "<img src=\"./images/YOLOv3_architecture_01.png\" alt=\"VGG16 architecture\" width=\"800\">\n",
    "\n",
    "**Source**: Ayoosh Kathuria, YOLOv3 &nbsp; &nbsp;\n",
    "**YOLO**: https://pjreddie.com/yolo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6.3 YOLOv3 Example - West Palm Beach\n",
    "\n",
    "### Clematis Street\n",
    "\n",
    "<img src=\"./images/wpb_clematis_01.jpg\" alt=\"Clematis Street\" width=\"600\">\n",
    "\n",
    "<center>Evening on Clematis Street</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6.3 YOLOv3 - Evening on Clematis Street\n",
    "\n",
    "<img src=\"./images/wpb_clematis_01_detected.jpg\" alt=\"Clematis Street\" width=\"600\">\n",
    "\n",
    "#### Predictions on image (total objects: 11)\n",
    "\n",
    "bus: 99.7612%\n",
    "person: 99.4076%\n",
    "person: 95.5676% <br/>\n",
    "bicycle: 99.6953%\n",
    "traffic light: 87.7151%\n",
    "traffic light: 84.3330% <br/>\n",
    "bus: 63.5284%\n",
    "car: 68.7173%\n",
    "car: 97.9441% <br/>\n",
    "car: 91.7072%\n",
    "motorbike: 67.3111%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 6.4 Other object detection architectures\n",
    "\n",
    "\n",
    "### Region CNN (R-CNN, 2015 - 2018)\n",
    "\n",
    "Series of models\n",
    "* Ujlinjs, 2011\n",
    "* R-CNN, 2014\n",
    "* Fast R-CNN, 2014\n",
    "* Faster R-CNN, 2014\n",
    "\n",
    "\n",
    "<img src=\"./images/FasterR-CNN_Object_detection_01.png\" alt=\"Faster R-CNN architecture\" width=\"800\">\n",
    "\n",
    "\n",
    "### SSD: Single-Shot Detector (20xx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6.5 Benchmarks and Performance Numbers\n",
    "\n",
    "\n",
    "## Image classification performance\n",
    "\n",
    "ImageNet 2011 - 2017, human performance (95%) vs. super-human performance (98%)\n",
    "(Source: [EFF AI Metrics](https://www.eff.org/ai/metrics))\n",
    "\n",
    "<img src=\"./images/CV-2017-ImageNetImageRecognition-EFFMetrics.png\" alt=\"CV-2017-ImageNet Image Recognition - EFF\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Object detection performance\n",
    "\n",
    "Human performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 7. New developments: New neural models\n",
    "\n",
    "### Feedback neural networks (inspired by the mamalian visual system)\n",
    "\n",
    "*Role of feedback in mammalian vision: a new hypothesis and a computational model*\n",
    "P.S. Sastry, Shesha Shah, S. Singh, K.P. Unnikrishnan, \n",
    "Vision Research 39 (1999) 131–148, Elsevier.\n",
    "\n",
    "*Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks*, Cao, Chunshui *et al.*, IEEE ICCV, 2015.\n",
    "[pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Cao_Look_and_Think_ICCV_2015_paper.pdf)\n",
    "[github](https://github.com/caochunshui/Feedback-CNN)\n",
    "[IEEE PAMI](https://www.computer.org/csdl/journal/tp/2019/07/08370896/13rRUwdIOTs)\n",
    "\n",
    "### Capsule networks\n",
    "\n",
    "*Dynamic Routing Between Capsules*,\n",
    "Sara Sabour, Nicholas Frosst, Geoffrey E Hinton, 2017 \n",
    "(https://arxiv.org/abs/1710.09829)\n",
    "(https://github.com/XifengGuo/CapsNet-Keras)\n",
    "\n",
    "Absolute MNIST error rate reduction from 0.21% (99.79% accuracy) \n",
    "to 0.17% (99.83% accuracy), Zhao *et al.*, 2019.\n",
    "\n",
    "*Capsule Networks with Max-Min Normalization*, \n",
    "Zhen Zhao, Ashley Kleinhans, Gursharan Sandhu, Ishan Patel, K. P. Unnikrishnan, 2019, \n",
    "(https://arxiv.org/abs/1903.09662)\n",
    "\n",
    "\n",
    "**NOTE**: After 30 years, **MNIST Test** can no longer be considered a proper *test* set (it is instead a *validation* set).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CapsNet with Max-Min: MNIST Test set errors\n",
    "\n",
    "\n",
    "Misclassified MNIST images using 3-model majority vote from CapsNets trained using Max-Min normalization\n",
    "\n",
    "<img src=\"./images/MNIST_maxmin_2019.png\" alt=\"Max-Min MNIST Test errors\" width=\"600\">\n",
    "\n",
    "* Total of 17 digit errors in 10,000 digit test set (99,83% test accuracy)\n",
    "* MNIST digit index 6576 - Ground Truth: \"7\"\n",
    "* current deep networks recognize as \"1\"; no human makes such error\n",
    "\n",
    "Source: [*Capsule Networks with Max-Min Normalization*, Zhao et al., 2019](https://arxiv.org/abs/1903.09662)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## New neural modules and networks for vision, cont'd\n",
    "\n",
    "\n",
    "### Feedback neural networks\n",
    "\n",
    "Deep networks with feedback from later layers inspired by the mamalian visual system.\n",
    "\n",
    "* *The Inversion of Sensory Processing by Feedback Pathways: A Model of Visual Cognitive Functions*, E. Harth; K. P. Unnikrishnan; A. S. Pandya, 1987, \n",
    "Science, New Series, Vol. 237, No. 4811. (Jul. 10, 1987), pp. 184-187. \n",
    "\n",
    "* *Role of feedback in mammalian vision: a new hypothesis and a computational model*\n",
    "P.S. Sastry, Shesha Shah, S. Singh, K.P. Unnikrishnan, \n",
    "Vision Research 39 (1999) 131–148, Elsevier Science.\n",
    "\n",
    "* *Feedback Convolutional Neural Network for Visual Localization and Segmentation*,\n",
    "C. Cao *et al.*, IEEE PAMI vol. 41, 2019.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this talk we have presented:\n",
    "\n",
    "* computer vision tasks of image classification and object detection\n",
    "\n",
    "* current image benchmark datasets (MNIST, Pascal VOC, ImageNet, MS-COCO)\n",
    "\n",
    "* the MLP and recent deep learning architectures\n",
    "\n",
    "* pre-trained, custom trained and used several deep learning architectures \n",
    "  * MLP and basic CNNs on MNIST\n",
    "  * VGG16 image classification on ImageNet\n",
    "  * YOLOv3 object detection on MS-COCO\n",
    "\n",
    "* Noted new developments (MNIST SOTA, capsules and feedback networks)\n",
    "\n",
    "\n",
    "## ... But, there are other considerations for computer vision and AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethics of AI: Other considerations for computer vision and AI\n",
    "\n",
    "There are many impacts that computer vision and AI already have in society:\n",
    "* The future of work (impact of CV and AI on jobs, leisure, income)\n",
    "  * What will the world economy be like when \"*AIs*\" running on wind and solar power do most of the work?\n",
    "* Surveillance and privacy (what are appropriate uses of CV and AI)\n",
    "* User manipulation and monetization \n",
    "  * e.g., election interference, tracking, advertisement, user behavior\n",
    "* Laws of robotics (I. Asimov, Robo-Cops, Drones, War machines)\n",
    "\n",
    "### Ethics of AI Initiatives\n",
    "\n",
    "* [MIT - AI and the Work of the Future](https://workofthefuturecongress.mit.edu/)\n",
    "* [Stanford Institute for Human-Centered Artificial Intelligence](https://hai.stanford.edu/)\n",
    "* [University of Oxford - Future of Humanity Institute](https://www.fhi.ox.ac.uk)\n",
    "* [Electronic Frontier Foundation - Artificial Intelligence & Machine Learning](https://www.eff.org/issues/ai)\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "We would to thank the following for useful discussions and comments on this work:\n",
    "* Johann Beukes and Brian Beam, *Levatas*\n",
    "* K.P. Unnikrishnan, *eNeuroLearn*\n",
    "\n",
    "<br/>\n",
    "\n",
    "*Slides prepared with* <br/>\n",
    "[<img src=\"./images/jupyter_logo.svg\" alt=\"Project Jupyter\" width=\"100\" align=\"left\">](https://jupyter.org/)\n",
    "\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "<br/>\n",
    "\n",
    "*Deep Learning with Python*, François Chollet, 2017, \n",
    "Manning Publications, (Chapter 5) <br/>\n",
    "https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\n",
    "\n",
    "\n",
    "*Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems*, Aurélien Géron, 2017, 1st ed. \n",
    "(second edition in October) <br/>\n",
    "https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291\n",
    "\n",
    "*Deep Learning*, LeCun, Y., Bengio, Y. and Hinton, G. E., Nature, Vol. 521, 2015\n",
    "[(pdf)](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)\n",
    "\n",
    "#### Keras\n",
    "* https://keras.io\n",
    "* http://github.com/keras-team/keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References, cont'd\n",
    "\n",
    "#### VGG16\n",
    "*Very Deep Convolutional Networks for Large-Scale Image Recognition*, \n",
    "Karen Simonyan & Andrew Zisserman, \n",
    "Visual Geometry Group, Department of Engineering Science, University of Oxford,\n",
    "ICLR 2015.\n",
    "(https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "#### YOLO\n",
    "*You Only Look Once: Unified, Real-Time Object Detection*, \n",
    "Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi; University of Washington, Allen Institute for AI, Facebook AI Research, 2015\n",
    "(https://pjreddie.com/yolo/)\n",
    "* https://arxiv.org/abs/1506.2640 (YOLO)\n",
    "* https://arxiv.org/abs/1612.08242 (YOLO-9000, aka YOLOv2)\n",
    "* https://arxiv.org/abs/1804.02767 (YOLOv3)\n",
    "\n",
    "\n",
    "#### Computer vision and object detection\n",
    "\n",
    "[*Robust Real-time Object Detection*, Paul Viola and Michael Jones, IJCV 2001](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-IJCV-01.pdf)\n",
    "\n",
    "*Learning OpenCV*, Gary Bradsky and Adrian Kaehler, O'Reilly, 2008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Thank you!**\n",
    "\n",
    "<img src=\"./images/dama_sentada_detected_crop.jpg\" alt=\"Dama Sentada\" width=\"450\" >\n",
    "\n",
    "<p/>\n",
    "\n",
    "\n",
    "<center>Dama Sentada, 1937 (Object detection by YOLOv3)</center>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
